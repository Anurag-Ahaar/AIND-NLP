{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Counting Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%writefile input.txt\n",
    "\n",
    "As I was waiting, a man came out of a side room, and at a glance I was sure he must be Long John. His left leg was cut off close by the hip, and under the left shoulder he carried a crutch, which he managed with wonderful dexterity, hopping about upon it like a bird. He was very tall and strong, with a face as big as a hamâ€”plain and pale, but intelligent and smiling. Indeed, he seemed in the most cheerful spirits, whistling as he moved about among the tables, with a merry word or a slap on the shoulder for the more favoured of his guests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(\"input.txt\", \"r\") as f:\n",
    "        text = f.read()\n",
    "        text= text.lower()\n",
    "        print(text)\n",
    "        \n",
    "        import re\n",
    "        pattern= \"[^a-zA-Z0-9]\"\n",
    "        text= re.split(pattern, text)\n",
    "        print(text)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Count words.\"\"\"\n",
    "\n",
    "def count_words(text):\n",
    "    \"\"\"Count how many times each unique word occurs in text.\"\"\"\n",
    "    counts = dict()  # dictionary of { <word>: <count> } pairs to return\n",
    "    \n",
    "    # TODO: Convert to lowercase\n",
    "    text= text.lower()\n",
    "    \n",
    "    # TODO: Split text into tokens (words), leaving out punctuation\n",
    "    # (Hint: Use regex to split on non-alphanumeric characters)\n",
    "    import re\n",
    "    pattern= \"[^a-z0-9]\"\n",
    "    text= re.split(pattern, text)\n",
    "    # remove pesky empty strings\n",
    "    text= filter(None, text)\n",
    "    \n",
    "    # TODO: Aggregate word counts using a dictionary\n",
    "    import collections\n",
    "    counts= collections.Counter()\n",
    "    counts.update(text)\n",
    "    \n",
    "    return counts\n",
    "\n",
    "\n",
    "def test_run(my_input_text):\n",
    "    with open(my_input_text, \"r\") as f:\n",
    "        text = f.read()\n",
    "        counts = count_words(text)\n",
    "        sorted_counts = sorted(counts.items(), key=lambda pair: pair[1], reverse=True)\n",
    "        \n",
    "        print(\"10 most common words:\\nWord\\tCount\")\n",
    "        for word, count in sorted_counts[:10]:\n",
    "            print(\"{}\\t{}\".format(word, count))\n",
    "        \n",
    "        print(\"\\n10 least common words:\\nWord\\tCount\")\n",
    "        for word, count in sorted_counts[-10:]:\n",
    "            print(\"{}\\t{}\".format(word, count))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_run(\"input.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%writefile test_case2.txt\n",
    "Buffalo buffalo Buffalo, buffalo buffalo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "my_string= \"Buffalo buffalo Buffalo, buffalo buffalo!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "my_string= my_string.lower()\n",
    "my_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pattern= \"[^a-z0-9]\"\n",
    "my_string= re.search(pattern, my_string)\n",
    "my_string.groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_run(\"test_case2.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Read Text Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Reading in text files.\"\"\"\n",
    "\n",
    "def read_file(filename):\n",
    "    \"\"\"Read a plain text file and return the contents as a string.\"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        text= f.read()\n",
    "        return text\n",
    "\n",
    "\n",
    "def read_files(path):\n",
    "    \"\"\"Read all files that match given path and return a dict with their contents.\"\"\"\n",
    "    import glob\n",
    "    import os\n",
    "    \n",
    "    #import pdb;pdb.set_trace()\n",
    "    # Get a list of all files and filenames\n",
    "    file_list= glob.glob(path)   \n",
    "    filenames= [os.path.basename(file_path) for file_path in file_list]\n",
    "    \n",
    "    \n",
    "    # Read each file using read_file()  and place assaemble dict of form { <filename>: <contents> }\n",
    "    contents= [read_file(file_path) for file_path in file_list]\n",
    "    \n",
    "    contents_dict= dict(zip(filenames, contents))\n",
    "    \n",
    "    return contents_dict\n",
    "\n",
    "\n",
    "def test_run():\n",
    "    # Test read_file()\n",
    "    print(read_file(\"data/hieroglyph.txt\"))\n",
    "    \n",
    "    # Test read_files()\n",
    "    texts = read_files(\"data/*.txt\")\n",
    "    for name in texts:\n",
    "        print(\"\\n***\", name, \"***\")\n",
    "        print(texts[name])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# %load bigram.py\n",
    "\"\"\"Bigram Model.\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import collections\n",
    "\n",
    "\n",
    "def read_file(filename):\n",
    "    \"\"\"Read a plain text file and return the contents as a string.\"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        text= f.read()\n",
    "        return text\n",
    "\n",
    "    \n",
    "\n",
    "def compute_bigram_model(path, files):\n",
    "    \"\"\"Compute a bigram model for a given corpus, including unigram probabilities.\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        path: directory where input files are located\n",
    "        files: list of files, or a single string specifying regex pattern to match (e.g. r'.*\\.txt')\n",
    "\n",
    "    Returns\n",
    "    =======\n",
    "        p_unigrams: dict with frequency of single words (need not be normalized to [0, 1])\n",
    "        p_bigrams: dict of dicts with frequency of bigrams (need not be normalized to [0, 1])\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Grab a list of all files in specified corpus\n",
    "    import pdb;pdb.set_trace()\n",
    "    if isinstance(files, str):\n",
    "        files = [f for f in os.listdir(path) if re.match(files, f)]  # collect all matching filenames\n",
    "    files = [os.path.join(path, f) for f in files]  # prepend path to each filename\n",
    "\n",
    "    # Read in text from each file and combine into a single string\n",
    "    filenames= [os.path.basename(file_path) for file_path in files]\n",
    "    contents= [read_file(file_path) for file_path in files]    \n",
    "    contents_dict= dict(zip(filenames, contents))    \n",
    "    \n",
    "\n",
    "    # Clean and tokenize text (note that you may want to retain case and sentence delimiters)\n",
    "    # Remove punctuation characters\n",
    "    import pdb;pdb.set_trace()\n",
    "    for key, val in contents_dict.items():\n",
    "        contents_dict[key]= re.sub(r\"[^a-zA-Z0-9]\", \" \", val).split()\n",
    "        words= contents_dict[key]\n",
    "        unigram_counts= collections.Counter()\n",
    "        unigram_counts.update(words)\n",
    "        total_counts= float(sum(unigram_counts.values()))\n",
    "        bigrams= collections.defaultdict(list)\n",
    "        \n",
    "        \n",
    "        # unigram probabilities       \n",
    "        #unigrams= [words[i] for i in range(len(words) - 1)]\n",
    "        for key, val in unigram_counts.items():\n",
    "            unigram_counts[key]\n",
    "            unigram_vals= unigram_counts.values()\n",
    "            unigram_elements= unigram_counts.elements()\n",
    "            unigram_probs= [value/total_counts for value in unigram_vals]\n",
    "        p_unigrams= dict(zip(unigram_elements, unigram_probs))\n",
    "        \n",
    "        # assemble bigrams dictionary\n",
    "        for i in range(len(words) - 1):\n",
    "            current_word= words[i]\n",
    "            next_word= words[i + 1]\n",
    "            bigrams[current_word].append(next_word)\n",
    "            \n",
    "        # convert to probabilities\n",
    "        p_bigrams= dict()\n",
    "        for word in bigrams.keys():\n",
    "            bigram_counts= collections.Counter()\n",
    "            bigram_counts.update(bigrams[word])\n",
    "            bigram_elements= bigram_counts.elements()\n",
    "            bigram_values= bigram_counts.values()\n",
    "            word_sum= float(sum(bigram_values))\n",
    "            bigram_probs= [value/word_sum for value in bigram_values]\n",
    "            p_bigrams[word]= dict(zip(bigram_elements, bigram_probs))\n",
    "            \n",
    "            \n",
    "\n",
    "    return p_unigrams, p_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generate_sequence(p_unigrams, p_bigrams, num_words=100, seed_word=None):\n",
    "    \"\"\"Generate a random sequence of words, given unigram and bigram probabilities.\"\"\"\n",
    "\n",
    "    # If seed_word is not given, pick one randomly based on unigram probabilities\n",
    "    if seed_word is None:\n",
    "        seed_word = random.choices(list(p_unigrams.keys()), weights=list(p_unigrams.values()))[0]\n",
    "    seq = [seed_word]\n",
    "    for i in range(num_words):\n",
    "        seq.append(random.choices(list(p_bigrams[seq[-1]].keys()), weights=list(p_bigrams[seq[-1]].values()))[0])\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-88-0cada4a6daf4>(35)compute_bigram_model()\n",
      "-> if isinstance(files, str):\n",
      "(Pdb) c\n",
      "> <ipython-input-88-0cada4a6daf4>(48)compute_bigram_model()\n",
      "-> for key, val in contents_dict.items():\n",
      "(Pdb) c\n"
     ]
    }
   ],
   "source": [
    "p_uni, p_bi= compute_bigram_model(path='.', files=['carroll-alice.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def test_run():\n",
    "    # Compute bigram model\n",
    "        p_unigrams, p_bigrams = compute_bigram_model(path='.', files=['carroll-alice.txt'])\n",
    "\n",
    "    # Check most common unigrams (single words)\n",
    "    print(\"10 most common unigrams:\")\n",
    "    sorted_unigrams = sorted(p_unigrams.items(), key=lambda item: item[1], reverse=True)  # each item = (i, count)\n",
    "    for word, count in sorted_unigrams[:10]:\n",
    "        print(\"{}\\t{}\".format(word, count))\n",
    "\n",
    "    # Check most common bigrams (pairs of words)\n",
    "    all_bigrams = [(i, j, count) for i in p_bigrams.keys() for j, count in p_bigrams[i].items()]\n",
    "    sorted_bigrams = sorted(all_bigrams, key=lambda item: item[2], reverse=True)  # each item = (i, j, count)\n",
    "    print(\"10 most common bigrams:\")\n",
    "    for i, j, count in sorted_bigrams[:10]:\n",
    "        print(\"{}\\t{}\\t{}\".format(i, j, count))\n",
    "\n",
    "    # Generate a sample sequence of words\n",
    "    seq = generate_sequence(p_unigrams, p_bigrams, seed_word=\"Alice\")\n",
    "    print(\" \".join(seq))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Example from Mentor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'cat', 'is', 'red', 'the', 'cat', 'is', 'green', 'the', 'cat', 'is', 'blue', 'the', 'dog', 'is', 'red']\n",
      "defaultdict(<type 'list'>, {'blue': ['the'], 'is': ['red', 'green', 'blue', 'red'], 'dog': ['is'], 'cat': ['is', 'is', 'is'], 'green': ['the'], 'the': ['cat', 'cat', 'cat', 'dog'], 'red': ['the']})\n",
      "defaultdict(<type 'list'>, {'blue': {'the': 1.0}, 'is': {'blue': 0.25, 'green': 0.25, 'red': 0.5}, 'dog': {'is': 1.0}, 'cat': {'is': 1.0}, 'green': {'the': 1.0}, 'the': {'dog': 0.25, 'cat': 0.75}, 'red': {'the': 1.0}})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "corpus = \"the cat is red the cat is green the cat is blue the dog is red\"\n",
    "\n",
    "tokenized_string = corpus.split()\n",
    "previous_word = \"\"\n",
    "dictionary = defaultdict(list)\n",
    "\n",
    "# Step 1: build Bigram dictionary\n",
    "for current_word in tokenized_string:\n",
    "  if previous_word != \"\":\n",
    "    dictionary[previous_word].append(current_word)\n",
    "  previous_word = current_word\n",
    "\n",
    "print(tokenized_string)\n",
    "print(dictionary)\n",
    "\n",
    "# Step 2: compute conditional probability\n",
    "for key in dictionary.keys():\n",
    "  next_words = dictionary[key]\n",
    "  unique_words = set(next_words)  # removes duplicated\n",
    "  nb_words = len(next_words)\n",
    "  cond_prob = {}\n",
    "  for unique_word in unique_words:\n",
    "    cond_prob[unique_word] = float(next_words.count(unique_word)) / nb_words\n",
    "  dictionary[key] = cond_prob\n",
    "\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'cat', 'is', 'red', 'the', 'cat', 'is', 'green', 'the', 'cat', 'is', 'blue', 'the', 'dog', 'is', 'red']\n",
      "defaultdict(<type 'list'>, {'blue': ['the'], 'is': ['red', 'green', 'blue', 'red'], 'dog': ['is'], 'cat': ['is', 'is', 'is'], 'green': ['the'], 'the': ['cat', 'cat', 'cat', 'dog'], 'red': ['the']})\n",
      "{'blue': {'the': 1.0}, 'is': {'blue': 0.25, 'green': 0.25, 'red': 0.5}, 'dog': {'is': 1.0}, 'cat': {'is': 1.0}, 'green': {'the': 1.0}, 'the': {'dog': 0.25, 'cat': 0.75}, 'red': {'the': 1.0}}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "corpus = \"the cat is red the cat is green the cat is blue the dog is red\"\n",
    "\n",
    "words = corpus.split()\n",
    "previous_word = \"\"\n",
    "bigrams = defaultdict(list)\n",
    "\n",
    "# Step 1: build Bigram dictionary\n",
    "#bigrams = [(words[i], words[i + 1]) for i in range(len(words) - 1)]\n",
    "\n",
    "for i in range(len(words) - 1):\n",
    "    current_word= words[i]\n",
    "    next_word= words[i + 1]\n",
    "    bigrams[current_word].append(next_word)\n",
    "    \n",
    "# Step 2: compute conditional probability\n",
    "p_bigrams= dict()\n",
    "for word in bigrams.keys():\n",
    "    counts= collections.Counter()\n",
    "    counts.update(bigrams[word])\n",
    "    elements= counts.elements()\n",
    "    values= counts.values()\n",
    "    word_sum= float(sum(values))\n",
    "    probs= [value/word_sum for value in values]\n",
    "    p_bigrams[word]= dict(zip(elements, probs))\n",
    "\n",
    "\n",
    "\n",
    "print(words)\n",
    "print(bigrams)\n",
    "print(p_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:aind-dl]",
   "language": "python",
   "name": "conda-env-aind-dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
